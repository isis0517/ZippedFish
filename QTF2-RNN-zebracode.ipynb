{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1313 unique characters.\n"
     ]
    }
   ],
   "source": [
    "text = np.load(\"fish_mur2.npy\")\n",
    "# The unique characters in the file\n",
    "vocabulary = sorted(set(text))\n",
    "print ('{} unique characters.'.format(len(vocabulary)))\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "\n",
    "c2emb = {char:index for index, char in enumerate(vocabulary)}\n",
    "emb2c = np.array(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 600\n",
    "examples_per_epoch = len(text)//sequence_length\n",
    "text_emb = np.array([c2emb[code] for code in text])\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 642  642  639  634  642  642  634  634  634  634  642  642  988  987\n",
      " 1212 1212 1213 1211 1213 1213 1213  985  985  985  985  985  985  642\n",
      "  642  642  642  642  647  642  642  642  642  642  647  642  988  984\n",
      " 1212 1213 1211 1211 1213 1213  985 1213 1213  985  985  985  985  985\n",
      "  985  985  985  985  985  642  642  642  642  642  642  642  642  642\n",
      "  642  642  962  957  954 1291 1193 1189  959  956  956  956  959  959\n",
      "  959  959  959  959  959  979  639  592  588  588  956  956  634  634\n",
      "  634  639  634  588  588  136  144  586  586  586  586  142  153  132\n",
      "  148  592  639  981 1209  987  984  641  641  700  700 1234 1234 1014\n",
      " 1015 1011  701  701 1011 1235 1011 1011 1011  701 1011  701  701  701\n",
      "  701  701  701  701  650  648  993  891 1109 1225 1003 1011 1011  701\n",
      "  695  695  701  695  695  701  695  695  701  695  695  695  695  695\n",
      "  260  695  642  986  986  983 1213 1213  984  985  985  985  642  642\n",
      "  642  642  642  642  695  642  695  642  202  260  847 1112  849  745\n",
      "  851  899 1253 1036  752  762  762  753  762  762  762  762  339  753\n",
      "  753  753 1037 1035 1035 1035  753  560  753 1018 1016  711  703 1013\n",
      " 1228 1235 1229 1229 1230 1230 1009 1011 1015 1015 1015  701 1015 1011\n",
      " 1011 1011 1009  993 1013 1228 1228 1235 1229 1010 1011 1011 1011 1011\n",
      " 1011 1015 1011 1015 1011  701  695  701  695  701  701  701  701  701\n",
      "  701 1033 1033 1033 1245 1251 1252 1018 1016 1252 1018 1037 1029 1034\n",
      "  709  744  753 1037 1037 1036 1251 1251 1252 1250 1250 1255 1252 1252\n",
      " 1252 1037 1037 1037 1037 1041 1037 1037 1037 1037 1037 1037  753  753\n",
      "  753  744  744  759  753  744  701 1011 1009 1009 1228 1147 1228 1010\n",
      " 1011 1029 1034 1029 1011 1029 1011 1029  706  744  701  744 1029  744\n",
      "  744  744  744  744  744  744  701  744  744  701  322  331  753 1073\n",
      "  801  801 1036 1267 1252 1037  751  753  939 1041 1037 1071  753  753\n",
      "  762  805  810 1082 1277 1273 1272 1272 1272 1278 1274 1278 1274 1079\n",
      " 1079 1079 1079 1079 1079 1079 1083 1079 1083 1079 1083  805  805 1079\n",
      " 1083 1044 1253 1253 1250 1255 1251 1251 1251 1252 1252 1252 1252 1252\n",
      " 1255 1252 1252 1252 1037 1252 1271 1278 1303 1303 1273 1278 1274 1272\n",
      " 1278 1278 1278 1278 1274 1278 1278 1278 1083 1083 1083 1278 1274 1275\n",
      " 1204 1278 1273 1273 1273 1274 1274 1274 1278 1278 1278 1278 1278 1304\n",
      " 1311 1311 1306 1307 1305 1307 1312 1307 1307 1307 1307 1307 1161 1167\n",
      " 1164 1163 1165 1165 1169 1165 1165 1165 1165 1165 1165 1165 1165 1165\n",
      " 1137 1289 1288 1309 1312 1306 1306 1307 1307 1307 1307 1307 1307 1307\n",
      " 1287 1290 1287 1287 1124 1124 1305 1125  868  868  868  459  865 1124\n",
      " 1127  871 1127 1086 1279 1305 1312 1306 1307 1307 1307 1312 1306 1307\n",
      " 1305 1305 1312 1306 1281 1279 1305 1312 1306 1306 1307 1312 1312 1312\n",
      " 1306 1312 1307 1312 1307 1307 1307 1307 1307 1307 1307 1307 1307 1124\n",
      " 1307 1124 1127 1312 1156 1160 1167 1164 1163 1165 1164 1165 1163 1163\n",
      " 1165 1169 1140 1137 1131 1128 1128 1134 1129 1129 1129 1134 1134 1134\n",
      " 1134 1134 1134 1134 1134 1130 1134 1134 1134  874  878  874  874  878\n",
      "  874  878  878  878  874  874  874  469  874  475  475  878  874]\n",
      "[ 823  771  763  763 1045 1045 1039 1039  751  751 1037  753  753  753\n",
      "  753  753  753  753 1037 1037 1019 1240 1240 1250 1255 1241 1238 1238\n",
      " 1252 1252 1255 1255 1252 1252 1252 1252 1041 1037 1037 1037 1037 1037\n",
      " 1037 1037 1238 1236 1232 1232 1230 1229 1229 1230 1230 1235 1235 1235\n",
      " 1235 1230 1230 1230 1230 1230 1230 1011 1230 1230 1230 1011 1011 1011\n",
      " 1011 1011 1011 1230 1222 1222 1228 1235 1143 1230 1228 1235 1229 1229\n",
      " 1230 1228 1230 1230 1235 1235 1230 1230 1230 1230 1230 1220 1222 1214\n",
      " 1211 1217 1212 1213 1223 1225 1225 1224 1212 1225 1213 1213 1213 1214\n",
      " 1214 1217 1212 1141 1212 1217 1217 1212 1213 1213 1213 1213 1213 1213\n",
      " 1194 1197 1191 1188 1193 1193 1193 1193 1190 1190 1193 1190 1193 1193\n",
      " 1190 1193 1193 1190 1190  959 1190 1190  956  959 1190  956  956  956\n",
      "  965  988 1212 1216 1212 1196 1211 1211 1217 1294 1213 1194 1194 1194\n",
      " 1194 1194 1196 1196 1196 1198 1196 1198 1198 1190 1170 1173 1173 1173\n",
      " 1170 1172 1165 1165 1169 1165 1171 1172  921  929  929  929  929  929\n",
      "  929  929  929  929  929  929  543  543  929  929  919 1166 1166 1163\n",
      " 1169 1164 1165 1165 1165 1165 1165 1169 1169  925 1165  921  921  921\n",
      "  921  921  921  921  921  921  921  535  879 1138 1166 1162 1158 1164\n",
      " 1165 1165 1169 1169 1169 1169 1169 1169 1169 1165 1165 1165  925  925\n",
      "  921  925  925  925  921  921  921  921  535  921  921  925  921  920\n",
      "  924 1164 1164 1165 1163 1163  919  921  921  921  921  921  921  535\n",
      "  535  921  921  540  535  535  535  881  879  883  827  827  823  823\n",
      "  823 1285 1090  821  819  404  404 1090 1305 1122  871  867  825  825\n",
      "  871  866 1123 1123  866  820  821  867 1123 1123 1092  874 1151  878\n",
      "  878  873  877  876  917  877  873  873 1130 1159  918 1134  878  878\n",
      "  881  884  921  482  535  482  475  875 1131 1131 1134 1134 1129 1129\n",
      " 1130 1130 1134  874  878  878  878  878  874  878  874  878  874  874\n",
      "  874  874  469  874 1092 1131 1128 1134 1129 1129 1130 1130 1130 1130\n",
      " 1130 1130 1130 1130 1134 1130  874  874  874  874  874  878  874  874\n",
      "  874  874  874  878  469  874  874  874  874  469 1092 1285 1125 1122\n",
      " 1122 1307 1307 1124 1122 1309 1305 1305 1312 1312 1307 1307 1307 1307\n",
      " 1124 1124 1124 1124 1124 1124 1124 1124 1124 1124 1124 1124 1124 1124\n",
      " 1124 1124  867  867  867  867  469  475  524  524  524  524   60  520\n",
      "  520  671  571  567  567  616  173  622  720  620  616  622  622  616\n",
      "  622  623  623  623  179  131  565  169  580  580  132  132  143  548\n",
      "  595  803  585 1242  980  697 1007 1014  990  893  650  650  650  648\n",
      "  648  985  647  641  985  985  983  640 1213 1212  984  985  985  983\n",
      "  985  985  985  985  642  642  985  642  642  647  642  642  642  642\n",
      "  642  642  642  202  212  694 1007  984 1213 1220  985  985  985  985\n",
      "  985  642  642  642  642  642  642  642  642  642  202  642  642  642\n",
      "  642  642  202  202  202  202  642  983  640  640 1008 1008  694  694\n",
      " 1003 1004 1011  695  642  260  642  491  496  647  642  642  642  202\n",
      "  202  650  701  698 1014 1010 1011 1011  701  699  699  701  499  701\n",
      "  695  701  701  701  701  701  701  695  695  701  268  260  701]\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(sequence_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(2):\n",
    "      print(item.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: ((64, 600), (64, 600)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "batch = 64\n",
    "steps_per_epoch = examples_per_epoch//batch\n",
    "\n",
    "# TF data maintains a buffer in memory in which to shuffle data\n",
    "# since it is designed to work with possibly endless data\n",
    "buffer = 10000\n",
    "\n",
    "dataset = dataset.shuffle(buffer).batch(batch, drop_remainder=True)\n",
    "\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The vocabulary length in characterrs\n",
    "vocabulary_length = len(vocabulary)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dimension = 512\n",
    "\n",
    "# Number of RNN units\n",
    "recurrent_nn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-a4ddce29a501>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU in use\n"
     ]
    }
   ],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    recurrent_nn = tf.compat.v1.keras.layers.CuDNNGRU\n",
    "    print(\"GPU in use\")\n",
    "else:\n",
    "    import functools\n",
    "    recurrent_nn = functools.partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')\n",
    "    print(\"CPU in use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocabulary_size, embedding_dimension, recurrent_nn_units, batch_size):\n",
    "    model = tf.keras.Sequential(\n",
    "        [tf.keras.layers.Embedding(vocabulary_size, embedding_dimension, batch_input_shape=[batch_size, None]),\n",
    "    recurrent_nn(recurrent_nn_units, return_sequences=True, recurrent_initializer='glorot_uniform', stateful=True),\n",
    "    tf.keras.layers.Dense(vocabulary_length)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocabulary_size = len(vocabulary),\n",
    "  embedding_dimension=embedding_dimension,\n",
    "  recurrent_nn_units=recurrent_nn_units,\n",
    "  batch_size=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_input_example, batch_target_example in dataset.take(1):\n",
    "    batch_predictions_example = model(batch_input_example)\n",
    "    print(batch_predictions_example.shape, \"# (batch, sequence_length, vocabulary_length)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 512)           672256    \n",
      "_________________________________________________________________\n",
      "cu_dnngru (CuDNNGRU)         (64, None, 1024)          4724736   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (64, None, 1313)          1345825   \n",
      "=================================================================\n",
      "Total params: 6,742,817\n",
      "Trainable params: 6,742,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(logits=batch_predictions_example[0], num_samples=1)\n",
    "\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1037, 1255, 1039, 1016,  328,  933,  280,  183,  329,  336,  469,\n",
       "       1094,  873,  819,  877, 1287,  469,  878,  874,  874,  877,  823,\n",
       "        469, 1238,  819, 1088,  406,  821,  821,  821,  821,  821,  469,\n",
       "        469, 1287,  874,  521, 1124,  406,  821,  821, 1307, 1124,  867,\n",
       "       1305,  461,  324,  821,  867,  406, 1305,  867,  867,  461, 1286,\n",
       "        956, 1275, 1079,  810, 1274, 1278, 1275,  918,  762,  810,    8,\n",
       "        741,  762,  837,  860,  580, 1152,  268,  753,  827,   36,  412,\n",
       "       1250, 1309, 1028,  743,  751, 1238,  744,  709, 1232, 1235, 1018,\n",
       "        752,  707,  487, 1177, 1015,  525,  709,  697,  772, 1037,  753,\n",
       "        753,  751,  744,  855,  751,  744, 1232, 1036, 1037,  760,  173,\n",
       "        852,  744, 1033, 1216, 1017, 1253,  705, 1011,  267, 1009])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 120, 1313)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       3.2654853\n"
     ]
    }
   ],
   "source": [
    "batch_loss_example  = tf.compat.v1.losses.sparse_softmax_cross_entropy(batch_target_example, batch_predictions_example)\n",
    "print(\"Prediction shape: \", batch_predictions_example.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", batch_loss_example.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next produced by upgrade script.... \n",
    "#model.compile(optimizer = tf.compat.v1.train.AdamOptimizer(), loss = loss) \n",
    "#.... but following optimizer is available.\n",
    "model.compile(optimizer = tf.optimizers.Adam(), loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "directory = './checkpoints-copy'\n",
    "# Name of the checkpoint files\n",
    "file_prefix = os.path.join(directory, \"ckpt_{epoch}\")\n",
    "\n",
    "callback=[tf.keras.callbacks.ModelCheckpoint(filepath=file_prefix, save_weights_only=True)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expect x to be a non-empty array or dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-e20e76b56c49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expect x to be a non-empty array or dataset.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expect x to be a non-empty array or dataset."
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints-copy/ckpt_300'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocabulary_size, embedding_dimension, recurrent_nn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(directory))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (1, None, 512)            672256    \n",
      "_________________________________________________________________\n",
      "cu_dnngru_2 (CuDNNGRU)       (1, None, 1024)           4724736   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 1313)           1345825   \n",
      "=================================================================\n",
      "Total params: 6,742,817\n",
      "Trainable params: 6,742,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, temperature, characters_to_generate):\n",
    "\n",
    "  # Vectorise  start string into numbers\n",
    "    input_string = [c2emb[code] for code in start_string]\n",
    "    input_string = tf.expand_dims(input_string, 0)\n",
    "\n",
    "  # Empty string to store  generated text\n",
    "    generated = start_string\n",
    "    \n",
    "  # (Batch size is 1)\n",
    "    model.reset_states()\n",
    "    for i in range(characters_to_generate):\n",
    "        predictions = model(input_string)\n",
    "      # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(logits=predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # Pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "        input_string = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        generated.append(emb2c[predicted_id])\n",
    "\n",
    "    return generated # generated is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate_text(model=model, start_string=[text[np.random.randint(len(text))]], temperature=0.1, characters_to_generate = 12000)\n",
    "#print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"fish_murme2.npy\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21486"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
